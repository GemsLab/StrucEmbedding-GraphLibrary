{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np, scipy as sp, networkx as nx\n",
    "import math, time, os, sys, random\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import scipy\n",
    "import scipy.sparse as sps\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import svds, eigs\n",
    "import sparsesvd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import sklearn.model_selection\n",
    "from time import time\n",
    "from sklearn.decomposition import NMF, DictionaryLearning\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "from scipy.stats import rankdata\n",
    "from scipy.stats import kendalltau\n",
    "from matplotlib import animation as anim\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    list_GT = [\n",
    "        (min((int(i.split(' ')[0]), int(i.split(' ')[1]))),\n",
    "         max((int(i.split(' ')[0]), int(i.split(' ')[1])))) for i in lines]\n",
    "    print(\"Read in\", len(list_GT), \"ground truth edges.\")\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(list_GT)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_label(filename: str):\n",
    "    dict_labels = dict()\n",
    "    dict_counter = dict()\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        dict_labels[int(line.split(' ')[0])] = int(line.split(' ')[1])\n",
    "        if int(line.split(' ')[1]) not in dict_counter:\n",
    "            dict_counter[int(line.split(' ')[1])] = 1\n",
    "        else:\n",
    "            dict_counter[int(line.split(' ')[1])] += 1\n",
    "    print(\"Read in\", len(dict_labels), 'node labels.')\n",
    "    for key, val in dict_counter.items():\n",
    "        print(\">>> Label\", key, 'appears', val, 'times')\n",
    "    return dict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldResults(X, y):\n",
    "    dict_performance_ = dict()\n",
    "    dict_prediction_ = dict()\n",
    "    kf = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    kf.get_n_splits(X)\n",
    "    for idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#         print('Fold:', idx)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "#         clf = LogisticRegression(random_state=0, solver='lbfgs', penalty='l2', C=1.0)\n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', penalty='l2', C=1.0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        dict_prediction_[idx] = y_pred\n",
    "\n",
    "        acc = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1_micro = metrics.f1_score(y_test, y_pred, average='micro')\n",
    "        f1_macro = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "        dict_ = {'acc': acc, 'f1_macro': f1_macro, 'f1_micro': f1_micro}\n",
    "        dict_performance_[idx] = dict_\n",
    "#         print(\"\\t>>> Accuracy:\", acc)\n",
    "#         print(\"\\t>>> F1_micro\", f1_micro)\n",
    "#         print(\"\\t>>> F1_macro\", f1_macro)\n",
    "    return {\"Performance\": dict_performance_, \"Prediction\": dict_prediction_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_best_result(X, y, n_clusters):\n",
    "    list_purity = list()\n",
    "    list_nmi = list()\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=5000, init='k-means++').fit(X)\n",
    "    \n",
    "    list_purity += [purity_score(y, kmeans.labels_)]\n",
    "    list_nmi += [normalized_mutual_info_score(y, kmeans.labels_)]\n",
    "    return {'purity': list_purity, 'nmi': list_nmi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(X, y):\n",
    "    dict_kfold_result_ = kFoldResults(X, y)\n",
    "\n",
    "    print(\">>>>>>>>>\")\n",
    "\n",
    "    current_metric = 'acc'\n",
    "    print('accuracy')\n",
    "    print(\"mean:\", '%.4f'%np.mean([dict_kfold_result_['Performance'][i][current_metric] for i in range(5)]))\n",
    "    print(\"std:\", '%.4f'%np.std([dict_kfold_result_['Performance'][i][current_metric] for i in range(5)]))\n",
    "\n",
    "    current_metric = 'f1_macro'\n",
    "    print('f1-macro')\n",
    "    print(\"mean:\", '%.4f'%np.mean([dict_kfold_result_['Performance'][i][current_metric] for i in range(5)]))\n",
    "    print(\"std:\", '%.4f'%np.std([dict_kfold_result_['Performance'][i][current_metric] for i in range(5)]))\n",
    "\n",
    "    print(\">>>>>>>>>\")\n",
    "    \n",
    "    return (np.mean([dict_kfold_result_['Performance'][i]['acc'] for i in range(5)]), \n",
    "            np.std([dict_kfold_result_['Performance'][i]['acc'] for i in range(5)]),\n",
    "            np.mean([dict_kfold_result_['Performance'][i]['f1_macro'] for i in range(5)]), \n",
    "            np.std([dict_kfold_result_['Performance'][i]['f1_macro'] for i in range(5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 131 node labels.\n",
      ">>> Label 0 appears 32 times\n",
      ">>> Label 1 appears 32 times\n",
      ">>> Label 3 appears 35 times\n",
      ">>> Label 2 appears 32 times\n"
     ]
    }
   ],
   "source": [
    "dict_labels = read_in_label('./sample-data/labels/airport_Brazil_label.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1074 ground truth edges.\n"
     ]
    }
   ],
   "source": [
    "G = read_graph('./sample-data/Airports/airport_Brazil/brazil-airports.edgelist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_degree = dict(G.degree())\n",
    "list_node_id = sorted([i for i in G.nodes()])\n",
    "list_emb = np.array([float(dict_degree[i]) for i in list_node_id]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>\n",
      "accuracy\n",
      "mean: 0.7413\n",
      "std: 0.0852\n",
      "f1-macro\n",
      "mean: 0.7294\n",
      "std: 0.0896\n",
      ">>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "X = np.array(list_emb).reshape(-1, 1)\n",
    "rtn = evaluate_classification(X, y)\n",
    "\n",
    "list_euclidean = list()\n",
    "list_cosine = list()\n",
    "list_best_nmi = list()\n",
    "list_best_purity = list()\n",
    "list_acc_mean = list()\n",
    "list_acc_std = list()\n",
    "list_f1macro_mean = list()\n",
    "list_f1macro_std = list()\n",
    "\n",
    "list_acc_mean += [rtn[0]]\n",
    "list_acc_std += [rtn[1]]\n",
    "list_f1macro_mean += [rtn[2]]\n",
    "list_f1macro_std += [rtn[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([dict_labels[i] for i in list_node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 3, 2, 3, 0, 3, 3, 1, 1, 3, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 1, 3, 3, 0, 2, 1, 1, 1, 0, 1, 2,\n",
       "       3, 1, 2, 2, 1, 1, 0, 0, 1, 0, 1, 2, 2, 2, 0, 2, 1, 0, 2, 1, 0, 1,\n",
       "       0, 3, 3, 0, 1, 0, 3, 3, 0, 1, 2, 0, 2, 0, 1, 2, 1, 2, 1, 2, 2, 1,\n",
       "       3, 2, 2, 2, 2, 1, 1, 3, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3,\n",
       "       3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 1, 3, 3, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40.],\n",
       "       [46.],\n",
       "       [63.],\n",
       "       [75.],\n",
       "       [48.],\n",
       "       [58.],\n",
       "       [59.],\n",
       "       [70.],\n",
       "       [13.],\n",
       "       [47.],\n",
       "       [40.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 4.],\n",
       "       [ 1.],\n",
       "       [36.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [15.],\n",
       "       [14.],\n",
       "       [ 1.],\n",
       "       [30.],\n",
       "       [16.],\n",
       "       [16.],\n",
       "       [41.],\n",
       "       [81.],\n",
       "       [ 7.],\n",
       "       [44.],\n",
       "       [18.],\n",
       "       [22.],\n",
       "       [33.],\n",
       "       [30.],\n",
       "       [ 6.],\n",
       "       [ 9.],\n",
       "       [ 3.],\n",
       "       [ 3.],\n",
       "       [37.],\n",
       "       [10.],\n",
       "       [11.],\n",
       "       [14.],\n",
       "       [21.],\n",
       "       [24.],\n",
       "       [24.],\n",
       "       [ 9.],\n",
       "       [ 2.],\n",
       "       [39.],\n",
       "       [15.],\n",
       "       [ 5.],\n",
       "       [14.],\n",
       "       [16.],\n",
       "       [44.],\n",
       "       [55.],\n",
       "       [ 8.],\n",
       "       [28.],\n",
       "       [17.],\n",
       "       [10.],\n",
       "       [ 8.],\n",
       "       [10.],\n",
       "       [46.],\n",
       "       [ 5.],\n",
       "       [12.],\n",
       "       [23.],\n",
       "       [ 4.],\n",
       "       [23.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [31.],\n",
       "       [28.],\n",
       "       [12.],\n",
       "       [25.],\n",
       "       [14.],\n",
       "       [33.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [28.],\n",
       "       [18.],\n",
       "       [10.],\n",
       "       [26.],\n",
       "       [ 4.],\n",
       "       [31.],\n",
       "       [12.],\n",
       "       [ 5.],\n",
       "       [12.],\n",
       "       [10.],\n",
       "       [38.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [16.],\n",
       "       [ 3.],\n",
       "       [ 9.],\n",
       "       [ 6.],\n",
       "       [ 9.],\n",
       "       [ 5.],\n",
       "       [10.],\n",
       "       [14.],\n",
       "       [ 3.],\n",
       "       [ 9.],\n",
       "       [17.],\n",
       "       [10.],\n",
       "       [ 5.],\n",
       "       [ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 7.],\n",
       "       [ 3.],\n",
       "       [ 8.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 4.],\n",
       "       [ 1.],\n",
       "       [ 6.],\n",
       "       [13.],\n",
       "       [ 3.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [ 4.],\n",
       "       [ 8.],\n",
       "       [ 3.],\n",
       "       [ 8.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 6.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_hop_histogram(G):\n",
    "    dict_degree = dict(G.degree())\n",
    "    dict_neighbors = {i:[ii for ii in G.neighbors(i)] for i in G.nodes()}\n",
    "    max_degree = max([i for _,i in dict_degree.items()])\n",
    "    dict_histogram = dict()\n",
    "    for cur_node in G.nodes():\n",
    "        list_histogram = [0.0] * max_degree\n",
    "        for cur_n in dict_neighbors[cur_node]:\n",
    "            for cur_nn in dict_neighbors[cur_n]:\n",
    "                list_histogram[dict_degree[cur_nn] - 1] += 1.0\n",
    "        dict_histogram[cur_node] = list_histogram\n",
    "    return dict_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>\n",
      "accuracy\n",
      "mean: 0.5499\n",
      "std: 0.0907\n",
      "f1-macro\n",
      "mean: 0.5086\n",
      "std: 0.1050\n",
      ">>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "dict_two_hop = two_hop_histogram(G)\n",
    "list_emb = np.array([dict_two_hop[i] for i in list_node_id])\n",
    "X = np.array(list_emb)\n",
    "rtn = evaluate_classification(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
